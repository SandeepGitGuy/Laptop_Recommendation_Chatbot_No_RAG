{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install google-generativeai\n",
    "%pip install openai\n",
    "import google.generativeai as genai\n",
    "import openai\n",
    "import getpass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Enter your Gemini API key: Please Note this is a demo version, only Gemini is currently supported\")\n",
    "API_KEY = getpass.getpass()\n",
    "genai.configure(api_key=API_KEY)\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "#response = model.generate_content(\"Write a story about a magic backpack\")\n",
    "#print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Laptop Recommendation Chatbot! Please Note this is a demo version. \n",
      "Bot:  Hi there! ðŸ‘‹  I'm here to help you find the perfect laptop.  What are you looking for in a laptop? Tell me about your needs and I'll do my best to guide you to the right choice. ðŸ˜Š \n",
      "\n",
      "You:  I am looking for gaming laptop\n",
      "Okay, a gaming laptop! That's great.  To help me find the perfect one for you, can you tell me your budget in INR? \n",
      "Not Done \n",
      "\n",
      "bot: Okay, a gaming laptop! That's great.  To help me find the perfect one for you, can you tell me your budget in INR? \n",
      "Okay, a gaming laptop! That's great.  To help me find the perfect one for you, can you tell me your budget in INR? \n",
      "Not Done \n",
      "\n",
      "user: \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid input: 'content' argument must not be empty. Please provide a non-empty value.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 98\u001b[0m\n\u001b[0;32m     95\u001b[0m     collect_info(user_input, history)\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;66;03m#print(history)\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m initialize_conversation()\n",
      "Cell \u001b[1;32mIn[70], line 95\u001b[0m, in \u001b[0;36minitialize_conversation\u001b[1;34m()\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou: \u001b[39m\u001b[38;5;124m\"\u001b[39m, user_input)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m#mod_resp = moderate_response(response)\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m#print(mod_resp)\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# print(\"Bot: \", response)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# response, history = chatbot_comm(user_input, history)\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# print(\"Bot: \", response)\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m collect_info(user_input, history)\n",
      "Cell \u001b[1;32mIn[70], line 70\u001b[0m, in \u001b[0;36mcollect_info\u001b[1;34m(user_input, history)\u001b[0m\n\u001b[0;32m     68\u001b[0m user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m()\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser:\u001b[39m\u001b[38;5;124m\"\u001b[39m, user_input)\n\u001b[1;32m---> 70\u001b[0m response, history \u001b[38;5;241m=\u001b[39m chatbot_comm(user_input, history)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbot:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m#print('test 1 ------------------------------')\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m#print(response)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[70], line 12\u001b[0m, in \u001b[0;36mchatbot_comm\u001b[1;34m(user_input, history)\u001b[0m\n\u001b[0;32m      6\u001b[0m chat \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mstart_chat(\n\u001b[0;32m      7\u001b[0m history \u001b[38;5;241m=\u001b[39m history\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m      9\u001b[0m generation_config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.1\u001b[39m  \u001b[38;5;66;03m# Adjust temperature for more deterministic (lower) or more creative (higher) responses\u001b[39;00m\n\u001b[0;32m     11\u001b[0m }\n\u001b[1;32m---> 12\u001b[0m response \u001b[38;5;241m=\u001b[39m chat\u001b[38;5;241m.\u001b[39msend_message(user_input, generation_config\u001b[38;5;241m=\u001b[39mgeneration_config)\n\u001b[0;32m     13\u001b[0m history\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparts\u001b[39m\u001b[38;5;124m\"\u001b[39m: [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_input}]\n\u001b[0;32m     16\u001b[0m })\n\u001b[0;32m     17\u001b[0m history\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparts\u001b[39m\u001b[38;5;124m\"\u001b[39m: [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: response\u001b[38;5;241m.\u001b[39mtext}]\n\u001b[0;32m     20\u001b[0m })\n",
      "File \u001b[1;32mc:\\Users\\sandy\\anaconda3\\Lib\\site-packages\\google\\generativeai\\generative_models.py:564\u001b[0m, in \u001b[0;36mChatSession.send_message\u001b[1;34m(self, content, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported configuration: The `google.generativeai` SDK currently does not support the combination of `stream=True` and `enable_automatic_function_calling=True`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    560\u001b[0m     )\n\u001b[0;32m    562\u001b[0m tools_lib \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39m_get_tools_lib(tools)\n\u001b[1;32m--> 564\u001b[0m content \u001b[38;5;241m=\u001b[39m content_types\u001b[38;5;241m.\u001b[39mto_content(content)\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m content\u001b[38;5;241m.\u001b[39mrole:\n\u001b[0;32m    567\u001b[0m     content\u001b[38;5;241m.\u001b[39mrole \u001b[38;5;241m=\u001b[39m _USER_ROLE\n",
      "File \u001b[1;32mc:\\Users\\sandy\\anaconda3\\Lib\\site-packages\\google\\generativeai\\types\\content_types.py:286\u001b[0m, in \u001b[0;36mto_content\u001b[1;34m(content)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_content\u001b[39m(content: ContentType):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m content:\n\u001b[1;32m--> 286\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    287\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m argument must not be empty. Please provide a non-empty value.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    288\u001b[0m         )\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content, Mapping):\n\u001b[0;32m    291\u001b[0m         content \u001b[38;5;241m=\u001b[39m _convert_dict(content)\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid input: 'content' argument must not be empty. Please provide a non-empty value."
     ]
    }
   ],
   "source": [
    "def chatbot_comm(user_input, history=[]):\n",
    "    chat = model.start_chat(\n",
    "    history = history\n",
    "    )\n",
    "    generation_config = {\n",
    "        \"temperature\": 0.1  # Adjust temperature for more deterministic (lower) or more creative (higher) responses\n",
    "    }\n",
    "    response = chat.send_message(user_input, generation_config=generation_config)\n",
    "    history.append({\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [{\"text\": user_input}]\n",
    "    })\n",
    "    history.append({\n",
    "        \"role\": \"model\",\n",
    "        \"parts\": [{\"text\": response.text}]\n",
    "    })\n",
    "    return response.text, history\n",
    "\n",
    "# Moderation is not working due to exceeded limit. Will implement later once billing is sorted.\n",
    "# def moderate_response(response):\n",
    "#     url = \"https://api.openai.com/v1/moderations\"\n",
    "#     from openai import OpenAI\n",
    "#     client=OpenAI(api_key=api_key)\n",
    "#     response = client.moderations.create(input=response)\n",
    "#     return response.results[0].flagged\n",
    "\n",
    "def collect_info(user_input, history=[]):\n",
    "    sys_message2 = \"\"\"\n",
    "    Remember to ask the questions one by one and wait for the user's response before asking the next question.\n",
    "    Keep asking them questions to understand their needs better. The Following data needs to be collected: \n",
    "    1. User's budget \n",
    "    2. GPU Intensity \n",
    "    3. Multitasking \n",
    "    4. Portability\n",
    "    5. Processing Speed\n",
    "    6. Display Quality\n",
    "    The Values for the above parameters should only be Low, Medium, High. Only Budget can be a number in INR. \n",
    "    You need to infer the values from the user's input as shown in the example below:\n",
    "    -- GPU Intensity : High\n",
    "    -- Multitasking : Medium\n",
    "    -- Portability : Low\n",
    "    -- Processing Speed : High\n",
    "    -- Display Quality : High\n",
    "    -- Budget : 50000\n",
    "    Keep asking questions until you have all the information you need. \n",
    "    in the response, do not include the above instructions or response.\n",
    "    If 'Not Done', continue asking questions.\n",
    "    if 'Done', display the collected information in the above format\n",
    "    Mandatorily respond with 'Done' or 'Not Done' after the response.\n",
    "    \"\"\"\n",
    "    combined_input = f\"{sys_message2}\\n\\nUser: {user_input}\"\n",
    "    response, history = chatbot_comm(combined_input, history)\n",
    "    print(\"bot:\", response.split('\\n')[0])\n",
    "    print(\"-\"*20)\n",
    "    # user_input = input()\n",
    "    # print(\"user:\", user_input)\n",
    "    while 'Not Done' in response:\n",
    "        user_input = input()\n",
    "        print(\"user:\", user_input)\n",
    "        response, history = chatbot_comm(user_input, history)\n",
    "        print(\"bot:\", response.split('\\n')[0])\n",
    "        print(\"-\"*20)\n",
    "        #print('test 1 ------------------------------')\n",
    "    print(response.split('\\n')[0])\n",
    "    return response\n",
    "\n",
    "def display_info(data):\n",
    "    sys_message3 = \"\"\"\n",
    "    You are the most intelligent laptop recommendation assistant.\n",
    "    The information collected from the user is as follows:\n",
    "    {data}\n",
    "    Use this information to recommend the best laptop for the user.\n",
    "    respond with the name of the laptop, its price, and and a small description of the laptop.\n",
    "    Respond with three best recommendations.\n",
    "    \"\"\"\n",
    "    response, history = chatbot_comm(sys_message3)\n",
    "    print(\"bot:\", response)\n",
    "    print(\"-\"*20)\n",
    "    \n",
    "\n",
    "\n",
    "def initialize_conversation():\n",
    "    print(\"Welcome to the Laptop Recommendation Chatbot! Please Note this is a demo version. \")\n",
    "    sys_message = \"\"\"You are a helpful assistant that can help user choose the best laptop for their needs. \n",
    "    Greet the user and ask how you can help them.\n",
    "    \"\"\"\n",
    "    #combined_input = f\"{sys_message}\\n\\nUser: {user_input}\"\n",
    "    response, history = chatbot_comm(sys_message)\n",
    "    print(\"Bot: \", response)\n",
    "    user_input = input()\n",
    "    print(\"You: \", user_input)\n",
    "    #mod_resp = moderate_response(response)\n",
    "    #print(mod_resp)\n",
    "    # print(\"Bot: \", response)\n",
    "    # user_input = input()\n",
    "    # print(\"You: \", user_input)\n",
    "    # response, history = chatbot_comm(user_input, history)\n",
    "    # print(\"Bot: \", response)\n",
    "    data = collect_info(user_input, history) \n",
    "    display_info(data)\n",
    "    #print(history)\n",
    "\n",
    "\n",
    "initialize_conversation()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
